{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Club Logo Classification — English Premier League\n",
    "\n",
    "**Deep Learning Final Project**\n",
    "\n",
    "Classifying 20 Premier League club logos using CNNs in PyTorch.\n",
    "\n",
    "**Dataset:** ~20,000 images across 20 clubs from [Kaggle](https://www.kaggle.com/datasets/alexteboul/english-premier-league-logo-detection-20k-images)\n",
    "\n",
    "**Structure:**\n",
    "- **Part 1** — Train custom CNNs from scratch, compare architectures/optimizers, tune hyperparameters, test regularization\n",
    "- **Part 2** — Transfer learning: pretrain on CIFAR-100, fine-tune on logos\n",
    "- **Part 3** — Fine-tune pretrained ResNet50 with different freezing strategies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup\nimport os\nif not os.path.exists('Premier-League-Logo-Classification'):\n    !git clone https://github.com/Guyisra26/Premier-League-Logo-Classification.git\n%cd Premier-League-Logo-Classification\n!pip install -q kagglehub\n\nfrom pathlib import Path\nfrom download_data import download_and_reset_data\n\nDATA_DIR = Path(\"data\")\nif not DATA_DIR.exists() or not any(DATA_DIR.iterdir()):\n    download_and_reset_data()\nelse:\n    print(f\"Dataset already exists at: {DATA_DIR}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from models import SimpleCNN, DeepCNN, get_resnet50\n",
    "from train_utils import set_seed, train_model, count_parameters\n",
    "from data import (find_data_dir, explore_dataset, get_cnn_transforms,\n",
    "                  get_cnn_transforms_no_aug, get_resnet_transforms,\n",
    "                  stratified_split, create_loaders, load_cifar100)\n",
    "from visualization import (plot_curves, plot_comparison, show_samples,\n",
    "                           plot_class_distribution, visualize_gradcam)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nSEED = 42\nBATCH_SIZE = 64\nNUM_CLASSES = 20\nNUM_EPOCHS = 12\nPATIENCE = 4\n\nset_seed(SEED)\nresults = {}\n\nWEIGHTS_DIR = './saved_weights'\nos.makedirs(WEIGHTS_DIR, exist_ok=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_DIR = find_data_dir('./data')\n",
    "class_names, class_counts = explore_dataset(DATA_DIR)\n",
    "plot_class_distribution(class_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stratified split: 70% train, 15% val, 15% test\n",
    "_, val_tf_cnn = get_cnn_transforms()\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=val_tf_cnn)\n",
    "class_names_dataset = full_dataset.classes\n",
    "print(f\"Classes: {class_names_dataset}\\n\")\n",
    "\n",
    "train_idx, val_idx, test_idx = stratified_split(full_dataset, seed=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DataLoaders for custom CNNs (128x128) and ResNet (224x224)\n",
    "train_tf_cnn, val_tf_cnn = get_cnn_transforms()\n",
    "train_tf_resnet, val_tf_resnet = get_resnet_transforms()\n",
    "\n",
    "train_loader_cnn, val_loader_cnn, test_loader_cnn = create_loaders(\n",
    "    DATA_DIR, train_idx, val_idx, test_idx,\n",
    "    train_tf_cnn, val_tf_cnn, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_loader_resnet, val_loader_resnet, test_loader_resnet = create_loaders(\n",
    "    DATA_DIR, train_idx, val_idx, test_idx,\n",
    "    train_tf_resnet, val_tf_resnet, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"CNN: {len(train_loader_cnn)} train batches, {len(val_loader_cnn)} val, {len(test_loader_cnn)} test\")\n",
    "print(f\"ResNet: {len(train_loader_resnet)} train batches\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample images\n",
    "sample_dataset = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf_cnn), val_idx[:500])\n",
    "show_samples(sample_dataset, class_names_dataset, n_per_class=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Training CNNs from Scratch\n",
    "\n",
    "I chose two architectures to compare:\n",
    "\n",
    "- **SimpleCNN** — 3 conv blocks (32→64→128), flat FC head. A minimal baseline.\n",
    "- **DeepCNN** — 5 conv blocks (32→64→128→256→256) with skip connections and Global Average Pooling. More capacity, but also more risk of overfitting.\n",
    "\n",
    "The idea is to see if the extra depth actually helps, or if a simpler model is enough for 20 logo classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check model architectures\n",
    "dummy = torch.randn(2, 3, 128, 128).to(device)\n",
    "\n",
    "print(\"SimpleCNN:\")\n",
    "m = SimpleCNN(NUM_CLASSES).to(device)\n",
    "count_parameters(m)\n",
    "print(f\"  Output: {m(dummy).shape}\\n\")\n",
    "\n",
    "print(\"DeepCNN:\")\n",
    "m = DeepCNN(NUM_CLASSES).to(device)\n",
    "count_parameters(m)\n",
    "print(f\"  Output: {m(dummy).shape}\")\n",
    "del m, dummy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Optimizer Comparison — Adam vs SGD+Momentum\n",
    "\n",
    "I'm comparing Adam (lr=1e-3) and SGD with momentum (lr=1e-2, StepLR scheduler) on both architectures. Everything else stays the same so the comparison is fair."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SimpleCNN + Adam\n",
    "set_seed(SEED)\n",
    "model_simple_adam = SimpleCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model_simple_adam.parameters(), lr=1e-3)\n",
    "results['SimpleCNN_Adam'] = train_model(\n",
    "    model_simple_adam, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(results['SimpleCNN_Adam'], 'SimpleCNN + Adam')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SimpleCNN + SGD\nset_seed(SEED)\nmodel_simple_sgd = SimpleCNN(NUM_CLASSES).to(device)\noptimizer = optim.SGD(model_simple_sgd.parameters(), lr=1e-2, momentum=0.9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nresults['SimpleCNN_SGD'] = train_model(\n    model_simple_sgd, train_loader_cnn, val_loader_cnn,\n    nn.CrossEntropyLoss(), optimizer, scheduler=scheduler,\n    epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\nplot_curves(results['SimpleCNN_SGD'], 'SimpleCNN + SGD')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DeepCNN + Adam\n",
    "set_seed(SEED)\n",
    "model_deep_adam = DeepCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model_deep_adam.parameters(), lr=1e-3)\n",
    "results['DeepCNN_Adam'] = train_model(\n",
    "    model_deep_adam, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(results['DeepCNN_Adam'], 'DeepCNN + Adam')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# DeepCNN + SGD\nset_seed(SEED)\nmodel_deep_sgd = DeepCNN(NUM_CLASSES).to(device)\noptimizer = optim.SGD(model_deep_sgd.parameters(), lr=1e-2, momentum=0.9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nresults['DeepCNN_SGD'] = train_model(\n    model_deep_sgd, train_loader_cnn, val_loader_cnn,\n    nn.CrossEntropyLoss(), optimizer, scheduler=scheduler,\n    epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\nplot_curves(results['DeepCNN_SGD'], 'DeepCNN + SGD')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare all 4 combinations\n",
    "plot_comparison(\n",
    "    [results['SimpleCNN_Adam'], results['SimpleCNN_SGD'],\n",
    "     results['DeepCNN_Adam'], results['DeepCNN_SGD']],\n",
    "    ['SimpleCNN+Adam', 'SimpleCNN+SGD', 'DeepCNN+Adam', 'DeepCNN+SGD'],\n",
    "    title='Architecture & Optimizer Comparison')\n",
    "\n",
    "for k in ['SimpleCNN_Adam', 'SimpleCNN_SGD', 'DeepCNN_Adam', 'DeepCNN_SGD']:\n",
    "    h = results[k]\n",
    "    print(f\"  {k:<20} val acc: {h['best_val_acc']:.4f}  (epoch {h['best_epoch']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Learning Rate Sweep\n",
    "\n",
    "Testing lr = {1e-2, 1e-3, 1e-4} with DeepCNN + Adam to find the best learning rate. I reuse the lr=1e-3 result from above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LR sweep\n",
    "lr_results = {'lr=1e-3': results['DeepCNN_Adam']}\n",
    "\n",
    "for lr in [1e-2, 1e-4]:\n",
    "    set_seed(SEED)\n",
    "    model = DeepCNN(NUM_CLASSES).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    key = f'lr={lr}'\n",
    "    results[f'DeepCNN_lr{lr}'] = train_model(\n",
    "        model, train_loader_cnn, val_loader_cnn,\n",
    "        nn.CrossEntropyLoss(), optimizer,\n",
    "        epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "    lr_results[key] = results[f'DeepCNN_lr{lr}']\n",
    "    del model\n",
    "\n",
    "plot_comparison(list(lr_results.values()), list(lr_results.keys()),\n",
    "                title='Learning Rate Sweep (DeepCNN + Adam)')\n",
    "for name, h in lr_results.items():\n",
    "    print(f\"  {name}: val acc = {h['best_val_acc']:.4f} (epoch {h['best_epoch']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1c: Batch Size Sweep\n",
    "\n",
    "Testing batch_size = {32, 64, 128}. Reusing bs=64 from above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Batch size sweep\n",
    "bs_results = {'bs=64': results['DeepCNN_Adam']}\n",
    "\n",
    "for bs in [32, 128]:\n",
    "    set_seed(SEED)\n",
    "    tl, vl, _ = create_loaders(DATA_DIR, train_idx, val_idx, test_idx,\n",
    "                                train_tf_cnn, val_tf_cnn, batch_size=bs)\n",
    "    model = DeepCNN(NUM_CLASSES).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    key = f'bs={bs}'\n",
    "    results[f'DeepCNN_bs{bs}'] = train_model(\n",
    "        model, tl, vl, nn.CrossEntropyLoss(), optimizer,\n",
    "        epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "    bs_results[key] = results[f'DeepCNN_bs{bs}']\n",
    "    del model, tl, vl\n",
    "\n",
    "plot_comparison(list(bs_results.values()), list(bs_results.keys()),\n",
    "                title='Batch Size Sweep (DeepCNN + Adam)')\n",
    "for name, h in bs_results.items():\n",
    "    print(f\"  {name}: val acc = {h['best_val_acc']:.4f} (epoch {h['best_epoch']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Architecture, Optimizers & Hyperparameters\n",
    "\n",
    "**Architecture:** DeepCNN outperformed SimpleCNN with both optimizers. The extra depth and skip connections helped the model learn more complex features for distinguishing the 20 logos. The skip connections are important because without them, 5 blocks would likely suffer from vanishing gradients.\n",
    "\n",
    "**Optimizers:** Adam converged faster than SGD in both architectures. This makes sense because Adam adapts the learning rate per parameter. SGD with the StepLR schedule was slower but still reached reasonable accuracy.\n",
    "\n",
    "**Learning rate:** The sweep showed that the default lr=1e-3 for Adam works well. lr=1e-2 was too aggressive (loss was unstable), and lr=1e-4 converged too slowly.\n",
    "\n",
    "**Batch size:** The differences between batch sizes were smaller than expected. Smaller batches (32) add more noise which can help regularize, while larger batches (128) train faster per epoch but may generalize slightly worse.\n",
    "\n",
    "**Overfitting:** Looking at the train vs val curves, the gap between training and validation accuracy grows over time — this is the classic sign of overfitting. The deeper model (DeepCNN) shows a bigger gap, which makes sense because it has more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Batch Normalization Ablation\n",
    "\n",
    "Training DeepCNN with and without BatchNorm to see how much it matters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DeepCNN without BatchNorm\n",
    "set_seed(SEED)\n",
    "model_deep_nobn = DeepCNN(NUM_CLASSES, use_batchnorm=False).to(device)\n",
    "count_parameters(model_deep_nobn)\n",
    "optimizer = optim.Adam(model_deep_nobn.parameters(), lr=1e-3)\n",
    "results['DeepCNN_NoBN'] = train_model(\n",
    "    model_deep_nobn, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "\n",
    "# Compare\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_NoBN']],\n",
    "    ['With BatchNorm', 'Without BatchNorm'],\n",
    "    title='BatchNorm Ablation')\n",
    "print(f\"  With BN:    {results['DeepCNN_Adam']['best_val_acc']:.4f}\")\n",
    "print(f\"  Without BN: {results['DeepCNN_NoBN']['best_val_acc']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Batch Normalization\n",
    "\n",
    "Removing BatchNorm from DeepCNN caused a noticeable drop in performance. The training was less stable (you can see the loss curve is more jittery) and it converged slower.\n",
    "\n",
    "This makes sense — BatchNorm normalizes the activations between layers, which helps with:\n",
    "- **Stability:** prevents activations from blowing up or vanishing\n",
    "- **Speed:** allows using higher learning rates\n",
    "- **Regularization:** the mini-batch statistics add a small amount of noise\n",
    "\n",
    "For a 5-block network like DeepCNN, BatchNorm is clearly important. A simpler 3-block model might get away without it, but the deeper you go, the more you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Regularization — Dropout, Weight Decay & Data Augmentation\n",
    "\n",
    "Testing three regularization techniques, one at a time:\n",
    "1. **Dropout** — rates 0.0, 0.3, 0.5\n",
    "2. **Weight decay** (L2) — 1e-4\n",
    "3. **Data augmentation** — with vs without"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dropout ablation\n",
    "for dr in [0.0, 0.3]:\n",
    "    set_seed(SEED)\n",
    "    model = DeepCNN(NUM_CLASSES, dropout_rate=dr).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    results[f'DeepCNN_dr{dr}'] = train_model(\n",
    "        model, train_loader_cnn, val_loader_cnn,\n",
    "        nn.CrossEntropyLoss(), optimizer,\n",
    "        epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "    del model\n",
    "\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_dr0.0'], results['DeepCNN_dr0.3'], results['DeepCNN_Adam']],\n",
    "    ['Dropout=0.0', 'Dropout=0.3', 'Dropout=0.5'],\n",
    "    title='Dropout Ablation')\n",
    "for dr, key in [('0.0','DeepCNN_dr0.0'), ('0.3','DeepCNN_dr0.3'), ('0.5','DeepCNN_Adam')]:\n",
    "    print(f\"  Dropout={dr}: {results[key]['best_val_acc']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weight decay\n",
    "set_seed(SEED)\n",
    "model_deep_wd = DeepCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model_deep_wd.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "results['DeepCNN_WD'] = train_model(\n",
    "    model_deep_wd, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_WD']],\n",
    "    ['No Weight Decay', 'Weight Decay=1e-4'],\n",
    "    title='Weight Decay Ablation')\n",
    "print(f\"  No WD:      {results['DeepCNN_Adam']['best_val_acc']:.4f}\")\n",
    "print(f\"  WD=1e-4:    {results['DeepCNN_WD']['best_val_acc']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data augmentation ablation\n",
    "set_seed(SEED)\n",
    "train_tf_noaug, val_tf_noaug = get_cnn_transforms_no_aug()\n",
    "tl_noaug, vl_noaug, _ = create_loaders(\n",
    "    DATA_DIR, train_idx, val_idx, test_idx,\n",
    "    train_tf_noaug, val_tf_noaug, batch_size=BATCH_SIZE)\n",
    "\n",
    "model_noaug = DeepCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model_noaug.parameters(), lr=1e-3)\n",
    "results['DeepCNN_NoAug'] = train_model(\n",
    "    model_noaug, tl_noaug, vl_noaug,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_NoAug']],\n",
    "    ['With Augmentation', 'Without Augmentation'],\n",
    "    title='Data Augmentation Ablation')\n",
    "print(f\"  With aug:    {results['DeepCNN_Adam']['best_val_acc']:.4f}\")\n",
    "print(f\"  Without aug: {results['DeepCNN_NoAug']['best_val_acc']:.4f}\")\n",
    "del model_noaug, tl_noaug, vl_noaug"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Regularization\n",
    "\n",
    "**Dropout:** Without any dropout (0.0), the model overfits more — the training accuracy goes very high but the validation accuracy doesn't follow. Dropout=0.5 gave the best generalization by randomly dropping neurons during training, which prevents the network from relying too heavily on specific features.\n",
    "\n",
    "**Weight decay:** Adding L2 regularization (weight_decay=1e-4) penalizes large weights and encourages simpler solutions. The effect was smaller than dropout — using both together might be too much regularization for this dataset size.\n",
    "\n",
    "**Data augmentation:** Training without augmentation led to faster overfitting. Without random flips, rotations, and color jitter, the model memorizes the training images instead of learning generalizable features. The train-val gap was much larger without augmentation.\n",
    "\n",
    "Overall, data augmentation had the biggest regularization impact, followed by dropout, and then weight decay."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Part 1 summary\n",
    "print(\"PART 1 SUMMARY\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Model':<28} {'Val Acc':>10} {'Epoch':>7} {'Time':>8}\")\n",
    "print(\"-\"*55)\n",
    "for name, key in [\n",
    "    ('SimpleCNN + Adam', 'SimpleCNN_Adam'),\n",
    "    ('SimpleCNN + SGD', 'SimpleCNN_SGD'),\n",
    "    ('DeepCNN + Adam', 'DeepCNN_Adam'),\n",
    "    ('DeepCNN + SGD', 'DeepCNN_SGD'),\n",
    "    ('DeepCNN (no BN)', 'DeepCNN_NoBN'),\n",
    "    ('DeepCNN dropout=0.0', 'DeepCNN_dr0.0'),\n",
    "    ('DeepCNN dropout=0.3', 'DeepCNN_dr0.3'),\n",
    "    ('DeepCNN + weight decay', 'DeepCNN_WD'),\n",
    "    ('DeepCNN (no augmentation)', 'DeepCNN_NoAug'),\n",
    "    ('DeepCNN lr=1e-2', 'DeepCNN_lr0.01'),\n",
    "    ('DeepCNN lr=1e-4', 'DeepCNN_lr0.0001'),\n",
    "    ('DeepCNN bs=32', 'DeepCNN_bs32'),\n",
    "    ('DeepCNN bs=128', 'DeepCNN_bs128'),\n",
    "]:\n",
    "    if key in results:\n",
    "        h = results[key]\n",
    "        print(f\"{name:<28} {h['best_val_acc']:>10.4f} {h['best_epoch']:>7} {h['training_time']:>7.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save best Part 1 models\n",
    "torch.save(model_deep_adam.state_dict(), os.path.join(WEIGHTS_DIR, 'deep_cnn_best.pth'))\n",
    "torch.save(model_simple_adam.state_dict(), os.path.join(WEIGHTS_DIR, 'simple_cnn_best.pth'))\n",
    "print(f\"Saved model weights to {WEIGHTS_DIR}/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Transfer Learning via CIFAR-100 Pretraining\n",
    "\n",
    "**Idea:** Pretrain the DeepCNN on CIFAR-100 (100 classes of natural images), save the weights, then fine-tune on our logo dataset with a smaller learning rate.\n",
    "\n",
    "**Why CIFAR-100?**\n",
    "- 100 classes forces the model to learn diverse features\n",
    "- It's structurally different from logos (photos vs graphics) — so this tests whether low-level features transfer across domains\n",
    "- CIFAR-100 over CIFAR-10 because more classes = richer feature representations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cifar_train_loader, cifar_val_loader = load_cifar100(batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pretrain on CIFAR-100\n",
    "set_seed(SEED)\n",
    "model_cifar = DeepCNN(num_classes=100).to(device)\n",
    "optimizer = optim.Adam(model_cifar.parameters(), lr=1e-3)\n",
    "history_cifar = train_model(\n",
    "    model_cifar, cifar_train_loader, cifar_val_loader,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(history_cifar, 'DeepCNN on CIFAR-100')\n",
    "\n",
    "# Save pretrained weights\n",
    "cifar_path = os.path.join(WEIGHTS_DIR, 'cifar100_pretrained.pth')\n",
    "torch.save(model_cifar.state_dict(), cifar_path)\n",
    "print(f\"Saved pretrained weights to {cifar_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load pretrained weights and fine-tune on logos\nset_seed(SEED)\nmodel_pretrained = DeepCNN(num_classes=NUM_CLASSES).to(device)\n\n# Load backbone weights (skip classifier since it has different output size)\npretrained_dict = torch.load(cifar_path, map_location=device, weights_only=True)\nmodel_dict = model_pretrained.state_dict()\npretrained_dict = {k: v for k, v in pretrained_dict.items()\n                   if k in model_dict and v.shape == model_dict[k].shape}\nmodel_dict.update(pretrained_dict)\nmodel_pretrained.load_state_dict(model_dict)\nprint(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} layers from CIFAR-100 model\")\n\n# Fine-tune with smaller learning rate\noptimizer = optim.Adam(model_pretrained.parameters(), lr=1e-4)\nresults['Pretrained'] = train_model(\n    model_pretrained, train_loader_cnn, val_loader_cnn,\n    nn.CrossEntropyLoss(), optimizer,\n    epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\nplot_curves(results['Pretrained'], 'CIFAR-100 Pretrained → Logos')\n\ntorch.save(model_pretrained.state_dict(), os.path.join(WEIGHTS_DIR, 'pretrained_finetuned.pth'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare pretrained vs from scratch\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['Pretrained']],\n",
    "    ['From Scratch', 'CIFAR-100 Pretrained'],\n",
    "    title='From Scratch vs Pretrained')\n",
    "print(f\"  From scratch: {results['DeepCNN_Adam']['best_val_acc']:.4f} (epoch {results['DeepCNN_Adam']['best_epoch']})\")\n",
    "print(f\"  Pretrained:   {results['Pretrained']['best_val_acc']:.4f} (epoch {results['Pretrained']['best_epoch']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: CIFAR-100 Transfer Learning\n",
    "\n",
    "The pretrained model converged faster in the early epochs — you can see it starts at a higher accuracy than the from-scratch model. This is because the convolutional layers already know how to detect edges, textures, and basic shapes from CIFAR-100.\n",
    "\n",
    "However, the final accuracy improvement was modest. This is likely because of the domain gap: CIFAR-100 contains natural photos (animals, vehicles, etc.), while our dataset is graphically designed logos. The low-level features (edges, colors) transfer well, but the higher-level features (object parts, textures) don't match.\n",
    "\n",
    "The key takeaway is that even pretraining on a mismatched domain helps somewhat — the model doesn't have to learn basic visual features from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Transfer Learning with Pretrained ResNet50\n",
    "\n",
    "ResNet50 pretrained on ImageNet (1.2M images, 1000 classes) should give a big boost. I'm testing three freezing strategies:\n",
    "\n",
    "1. **Fully frozen** — only train the new FC head (fast, tests if ImageNet features are enough)\n",
    "2. **Partial unfreeze** — unfreeze layer4 + head (let high-level features adapt)\n",
    "3. **Full fine-tune** — train everything with differential LR (backbone=1e-5, head=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ResNet50 - Fully Frozen\n",
    "set_seed(SEED)\n",
    "model_resnet_frozen = get_resnet50(NUM_CLASSES, 'full_freeze').to(device)\n",
    "count_parameters(model_resnet_frozen)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_resnet_frozen.parameters()), lr=1e-3)\n",
    "results['ResNet_Frozen'] = train_model(\n",
    "    model_resnet_frozen, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(results['ResNet_Frozen'], 'ResNet50 Frozen')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ResNet50 - Partial Unfreeze\n",
    "set_seed(SEED)\n",
    "model_resnet_partial = get_resnet50(NUM_CLASSES, 'partial').to(device)\n",
    "count_parameters(model_resnet_partial)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_resnet_partial.parameters()), lr=1e-4)\n",
    "results['ResNet_Partial'] = train_model(\n",
    "    model_resnet_partial, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(results['ResNet_Partial'], 'ResNet50 Partial Unfreeze')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ResNet50 - Full Fine-tune\n",
    "set_seed(SEED)\n",
    "model_resnet_full = get_resnet50(NUM_CLASSES, 'full_finetune').to(device)\n",
    "count_parameters(model_resnet_full)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': [p for n, p in model_resnet_full.named_parameters() if 'fc' not in n], 'lr': 1e-5},\n",
    "    {'params': model_resnet_full.fc.parameters(), 'lr': 1e-3},\n",
    "])\n",
    "results['ResNet_Full'] = train_model(\n",
    "    model_resnet_full, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer, epochs=NUM_EPOCHS, patience=PATIENCE, device=device)\n",
    "plot_curves(results['ResNet_Full'], 'ResNet50 Full Fine-tune')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare ResNet strategies\n",
    "plot_comparison(\n",
    "    [results['ResNet_Frozen'], results['ResNet_Partial'], results['ResNet_Full']],\n",
    "    ['Frozen', 'Partial', 'Full Fine-tune'],\n",
    "    title='ResNet50 Freezing Strategies')\n",
    "for k in ['ResNet_Frozen', 'ResNet_Partial', 'ResNet_Full']:\n",
    "    print(f\"  {k:<20} val acc: {results[k]['best_val_acc']:.4f} (epoch {results[k]['best_epoch']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save ResNet models\n",
    "for name, model in [('resnet_frozen', model_resnet_frozen),\n",
    "                    ('resnet_partial', model_resnet_partial),\n",
    "                    ('resnet_full', model_resnet_full)]:\n",
    "    torch.save(model.state_dict(), os.path.join(WEIGHTS_DIR, f'{name}.pth'))\n",
    "print(f\"Saved ResNet models to {WEIGHTS_DIR}/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: ResNet50\n",
    "\n",
    "Even the fully frozen ResNet50 (training only the last layer!) achieved strong results. This shows that the features learned on ImageNet are very transferable to logo classification — edges, shapes, textures, and color patterns are useful across domains.\n",
    "\n",
    "Partial unfreezing (layer4 + head) gave an improvement because it allows the higher-level features to adapt to logos specifically. The lower layers (which detect basic edges and textures) don't need to change much.\n",
    "\n",
    "Full fine-tuning with differential learning rates gave the best performance. Using a very small lr (1e-5) for the backbone prevents destroying the pretrained features, while the larger lr (1e-3) for the head lets it learn quickly.\n",
    "\n",
    "Compared to my custom DeepCNN, ResNet50 performed significantly better. This makes sense — it was trained on 1.2 million images and has 50 layers of learned representations. My custom model with ~3.5M parameters can't compete with ResNet50's 23.5M parameters and massive pretraining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Analysis & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cross-part comparison\n",
    "best_custom = max(['DeepCNN_Adam', 'DeepCNN_SGD'], key=lambda k: results[k]['best_val_acc'])\n",
    "best_resnet = max(['ResNet_Frozen', 'ResNet_Partial', 'ResNet_Full'], key=lambda k: results[k]['best_val_acc'])\n",
    "\n",
    "plot_comparison(\n",
    "    [results[best_custom], results['Pretrained'], results[best_resnet]],\n",
    "    [f'Part 1: {best_custom}', 'Part 2: Pretrained', f'Part 3: {best_resnet}'],\n",
    "    title='Best Model from Each Part')\n",
    "\n",
    "print(\"Best model from each part:\")\n",
    "for label, key in [('Part 1 (scratch)', best_custom),\n",
    "                   ('Part 2 (CIFAR PT)', 'Pretrained'),\n",
    "                   ('Part 3 (ResNet)', best_resnet)]:\n",
    "    h = results[key]\n",
    "    print(f\"  {label:<22} {key:<20} val acc={h['best_val_acc']:.4f} (ep {h['best_epoch']}, {h['training_time']:.0f}s)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test set evaluation with best model\n",
    "best_key = max(results, key=lambda k: results[k]['best_val_acc'])\n",
    "print(f\"Best overall model: {best_key} (val acc = {results[best_key]['best_val_acc']:.4f})\\n\")\n",
    "\n",
    "is_resnet = 'ResNet' in best_key\n",
    "test_loader = test_loader_resnet if is_resnet else test_loader_cnn\n",
    "\n",
    "model_map = {\n",
    "    'SimpleCNN_Adam': model_simple_adam, 'SimpleCNN_SGD': model_simple_sgd,\n",
    "    'DeepCNN_Adam': model_deep_adam, 'DeepCNN_SGD': model_deep_sgd,\n",
    "    'DeepCNN_NoBN': model_deep_nobn, 'DeepCNN_WD': model_deep_wd,\n",
    "    'Pretrained': model_pretrained,\n",
    "    'ResNet_Frozen': model_resnet_frozen,\n",
    "    'ResNet_Partial': model_resnet_partial,\n",
    "    'ResNet_Full': model_resnet_full,\n",
    "}\n",
    "\n",
    "best_model = model_map.get(best_key, model_resnet_full)\n",
    "best_model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = best_model(images.to(device))\n",
    "        all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "test_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_dataset, yticklabels=class_names_dataset)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix — {best_key} (Test Acc: {test_acc:.4f})')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-class metrics\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names_dataset))\n",
    "\n",
    "report = classification_report(all_labels, all_preds,\n",
    "                               target_names=class_names_dataset, output_dict=True)\n",
    "f1_scores = {cls: report[cls]['f1-score'] for cls in class_names_dataset}\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(f1_scores.keys(), f1_scores.values(), color='teal')\n",
    "plt.axhline(y=np.mean(list(f1_scores.values())), color='red', linestyle='--',\n",
    "            label=f\"Mean F1: {np.mean(list(f1_scores.values())):.3f}\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Per-Class F1 Scores')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grad-CAM visualization\n",
    "if 'ResNet' in best_key:\n",
    "    target_layer = best_model.layer4[-1]\n",
    "    grad_loader = test_loader_resnet\n",
    "else:\n",
    "    target_layer = best_model.block5 if hasattr(best_model, 'block5') else best_model.features[-1]\n",
    "    grad_loader = test_loader_cnn\n",
    "\n",
    "visualize_gradcam(best_model, target_layer, grad_loader,\n",
    "                  class_names_dataset, device, n_samples=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Test Results\n",
    "\n",
    "The confusion matrix shows that most classes are well-separated. Some clubs with visually similar logos (similar colors or circular shapes) get confused more often.\n",
    "\n",
    "The per-class F1 scores show which logos are easiest and hardest to classify. Clubs with very distinctive logos (unique colors, unusual shapes) have the highest F1, while clubs with more generic-looking crests are harder.\n",
    "\n",
    "The Grad-CAM heatmaps confirm that the model is looking at the actual logo features (crests, text, symbols) rather than background artifacts. This is a good sign that the model learned meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Reflection\n",
    "\n",
    "### What architectural choices mattered most\n",
    "\n",
    "1. **Pretrained weights** — The biggest factor by far. ResNet50 with ImageNet weights outperformed everything else. This shows that features learned on a large diverse dataset transfer well even to a domain-specific task like logo classification.\n",
    "\n",
    "2. **Network depth** — DeepCNN outperformed SimpleCNN, confirming that more layers with skip connections can learn richer representations. But the gap was much smaller than the gap between scratch and pretrained.\n",
    "\n",
    "3. **Batch Normalization** — Essential for the deeper model. Removing it caused noticeable degradation.\n",
    "\n",
    "4. **Regularization** — Dropout and data augmentation helped reduce overfitting, but their impact was secondary to architecture and pretraining.\n",
    "\n",
    "### When transfer learning helped\n",
    "\n",
    "- CIFAR-100 pretraining gave a modest boost — the low-level features transferred but the domain gap limited higher-level transfer.\n",
    "- ResNet50 (ImageNet) gave the strongest results because ImageNet is massive and diverse.\n",
    "- Even the frozen ResNet performed well, showing ImageNet features are broadly useful.\n",
    "\n",
    "### What I would do differently with more time\n",
    "\n",
    "- Try EfficientNet or Vision Transformers instead of ResNet50\n",
    "- Use a logo-specific pretraining dataset (like FlickrLogos-32) instead of CIFAR-100\n",
    "- Try CutMix/MixUp augmentation\n",
    "- Increase resolution beyond 128x128 for custom CNNs\n",
    "- Build an ensemble of the best custom CNN + ResNet50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}