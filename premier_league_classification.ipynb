{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Club Logo Classification — English Premier League\n",
    "\n",
    "**Deep Learning Final Project**\n",
    "\n",
    "This notebook implements multi-class image classification of English Premier League football club logos using Convolutional Neural Networks in PyTorch.\n",
    "\n",
    "**Dataset:** ~20,000 images across 20 Premier League clubs ([Kaggle](https://www.kaggle.com/datasets/alexteboul/english-premier-league-logo-detection-20k-images))\n",
    "\n",
    "**Project Structure:**\n",
    "\n",
    "| File | Description |\n",
    "|---|---|\n",
    "| `models.py` | CNN architectures: SimpleCNN, DeepCNN, ResNet50 wrapper |\n",
    "| `train_utils.py` | Training loop, evaluation, early stopping, seed utility |\n",
    "| `data.py` | Transforms, dataset loading, stratified splitting, DataLoaders |\n",
    "| `visualization.py` | Plotting functions, Grad-CAM implementation |\n",
    "| This notebook | Experiments, results, and analysis |\n",
    "\n",
    "**Experiments:**\n",
    "1. **Part 1** — Custom CNNs from scratch with optimizer comparison, BatchNorm ablation, dropout & weight decay regularization\n",
    "2. **Part 2** — Transfer learning via CIFAR-100 pretraining\n",
    "3. **Part 3** — Fine-tuning pretrained ResNet50 with different freezing strategies\n",
    "4. **Final Analysis** — Confusion matrix, per-class metrics, Grad-CAM, summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Setup: Clone repo & install dependencies ────────────────────────────────\n!git clone https://github.com/Guyisra26/Premier-League-Logo-Classification.git\n%cd Premier-League-Logo-Classification\n!pip install -q kagglehub\n\nfrom pathlib import Path\nfrom download_data import download_and_reset_data\n\nDATA_DIR = Path(\"data\")\nif not DATA_DIR.exists() or not any(DATA_DIR.iterdir()):\n    download_and_reset_data()\nelse:\n    print(f\"Dataset already exists at: {DATA_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ─────────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Project modules\n",
    "from models import SimpleCNN, DeepCNN, get_resnet50\n",
    "from train_utils import set_seed, train_model, count_parameters\n",
    "from data import (find_data_dir, explore_dataset, get_cnn_transforms,\n",
    "                  get_resnet_transforms, stratified_split, create_loaders,\n",
    "                  load_cifar100)\n",
    "from visualization import (plot_curves, plot_comparison, show_samples,\n",
    "                           plot_class_distribution, GradCAM, visualize_gradcam)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ───────────────────────────────────────────────────────────\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 20\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Store all experiment results\n",
    "results = {}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & Exploration\n",
    "\n",
    "The dataset contains ~20,000 images of English Premier League club logos, organized in class-named folders. We explore the distribution, define augmentation transforms, and create stratified train/val/test splits (70/15/15)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Discover and explore dataset\n",
    "DATA_DIR = find_data_dir('./data')\n",
    "class_names, class_counts = explore_dataset(DATA_DIR)\n",
    "plot_class_distribution(class_counts)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load full dataset for splitting, then create stratified indices\n",
    "from torchvision import datasets\n",
    "\n",
    "_, val_tf_cnn = get_cnn_transforms()\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=val_tf_cnn)\n",
    "class_names_dataset = full_dataset.classes\n",
    "\n",
    "print(f\"Classes: {class_names_dataset}\\n\")\n",
    "train_idx, val_idx, test_idx = stratified_split(full_dataset, seed=SEED)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create DataLoaders for custom CNNs (128x128) and ResNet50 (224x224)\n",
    "train_tf_cnn, val_tf_cnn = get_cnn_transforms()\n",
    "train_tf_resnet, val_tf_resnet = get_resnet_transforms()\n",
    "\n",
    "train_loader_cnn, val_loader_cnn, test_loader_cnn = create_loaders(\n",
    "    DATA_DIR, train_idx, val_idx, test_idx,\n",
    "    train_tf_cnn, val_tf_cnn, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_loader_resnet, val_loader_resnet, test_loader_resnet = create_loaders(\n",
    "    DATA_DIR, train_idx, val_idx, test_idx,\n",
    "    train_tf_resnet, val_tf_resnet, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"CNN loaders — Train: {len(train_loader_cnn)} batches, \"\n",
    "      f\"Val: {len(val_loader_cnn)}, Test: {len(test_loader_cnn)}\")\n",
    "print(f\"ResNet loaders — Train: {len(train_loader_resnet)} batches, \"\n",
    "      f\"Val: {len(val_loader_resnet)}, Test: {len(test_loader_resnet)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize sample images per class\n",
    "from torch.utils.data import Subset\n",
    "sample_dataset = Subset(\n",
    "    datasets.ImageFolder(DATA_DIR, transform=val_tf_cnn), val_idx[:500]\n",
    ")\n",
    "show_samples(sample_dataset, class_names_dataset, n_per_class=3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Training CNNs from Scratch\n",
    "\n",
    "We design two CNN architectures to isolate the effect of depth and skip connections:\n",
    "\n",
    "| Model | Blocks | Filters | Key Feature |\n",
    "|---|---|---|---|\n",
    "| **SimpleCNN** | 3 conv | 32→64→128 | Flat FC head, minimal capacity |\n",
    "| **DeepCNN** | 5 conv | 32→64→128→256→256 | Skip connections, GAP, configurable BN/Dropout |\n",
    "\n",
    "**Why these architectures?** SimpleCNN serves as a minimal baseline — can a simple feature extractor distinguish 20 logo classes? DeepCNN adds depth with skip connections to test whether richer feature hierarchies improve classification without vanishing gradient issues."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify model architectures\n",
    "dummy = torch.randn(2, 3, 128, 128).to(device)\n",
    "\n",
    "print(\"SimpleCNN:\")\n",
    "m = SimpleCNN(NUM_CLASSES).to(device)\n",
    "count_parameters(m)\n",
    "print(f\"  Output shape: {m(dummy).shape}\\n\")\n",
    "\n",
    "print(\"DeepCNN (BN=True, dropout=0.5):\")\n",
    "m = DeepCNN(NUM_CLASSES, use_batchnorm=True, dropout_rate=0.5).to(device)\n",
    "count_parameters(m)\n",
    "print(f\"  Output shape: {m(dummy).shape}\")\n",
    "\n",
    "del m, dummy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Optimizer Comparison — Adam vs SGD+Momentum\n",
    "\n",
    "We compare two widely used optimizers:\n",
    "- **Adam** (lr=1e-3, default betas): Adaptive per-parameter learning rates. Expected to converge faster.\n",
    "- **SGD+Momentum** (lr=1e-2, momentum=0.9, StepLR decay every 15 epochs): Fixed learning rate with momentum. Expected to generalize better but converge slower.\n",
    "\n",
    "Both optimizers are tested on both architectures, with all other hyperparameters held constant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── SimpleCNN + Adam ────────────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"SimpleCNN + Adam (lr=1e-3)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_simple_adam = SimpleCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model_simple_adam.parameters(), lr=1e-3)\n",
    "\n",
    "results['SimpleCNN_Adam'] = train_model(\n",
    "    model_simple_adam, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['SimpleCNN_Adam'], 'SimpleCNN + Adam')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── SimpleCNN + SGD+Momentum ────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"SimpleCNN + SGD+Momentum (lr=1e-2)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_simple_sgd = SimpleCNN(NUM_CLASSES).to(device)\n",
    "optimizer = optim.SGD(model_simple_sgd.parameters(), lr=1e-2, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "results['SimpleCNN_SGD'] = train_model(\n",
    "    model_simple_sgd, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, scheduler=scheduler,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['SimpleCNN_SGD'], 'SimpleCNN + SGD+Momentum')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── SimpleCNN comparison ────────────────────────────────────────────────────\n",
    "plot_comparison(\n",
    "    [results['SimpleCNN_Adam'], results['SimpleCNN_SGD']],\n",
    "    ['Adam', 'SGD+Momentum'],\n",
    "    title='SimpleCNN: Optimizer Comparison'\n",
    ")\n",
    "for k in ['SimpleCNN_Adam', 'SimpleCNN_SGD']:\n",
    "    h = results[k]\n",
    "    print(f\"  {k:<20} Best val acc: {h['best_val_acc']:.4f} (epoch {h['best_epoch']})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── DeepCNN + Adam ──────────────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"DeepCNN + Adam (lr=1e-3)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_deep_adam = DeepCNN(NUM_CLASSES, use_batchnorm=True, dropout_rate=0.5).to(device)\n",
    "optimizer = optim.Adam(model_deep_adam.parameters(), lr=1e-3)\n",
    "\n",
    "results['DeepCNN_Adam'] = train_model(\n",
    "    model_deep_adam, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['DeepCNN_Adam'], 'DeepCNN + Adam')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── DeepCNN + SGD+Momentum ──────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"DeepCNN + SGD+Momentum (lr=1e-2)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_deep_sgd = DeepCNN(NUM_CLASSES, use_batchnorm=True, dropout_rate=0.5).to(device)\n",
    "optimizer = optim.SGD(model_deep_sgd.parameters(), lr=1e-2, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "results['DeepCNN_SGD'] = train_model(\n",
    "    model_deep_sgd, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer, scheduler=scheduler,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['DeepCNN_SGD'], 'DeepCNN + SGD+Momentum')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── DeepCNN comparison ──────────────────────────────────────────────────────\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_SGD']],\n",
    "    ['Adam', 'SGD+Momentum'],\n",
    "    title='DeepCNN: Optimizer Comparison'\n",
    ")\n",
    "for k in ['DeepCNN_Adam', 'DeepCNN_SGD']:\n",
    "    h = results[k]\n",
    "    print(f\"  {k:<20} Best val acc: {h['best_val_acc']:.4f} (epoch {h['best_epoch']})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Architecture & Optimizer Comparison\n",
    "\n",
    "**Architecture observations:**\n",
    "- Compare SimpleCNN vs DeepCNN with the same optimizer — the deeper model should show higher capacity and better final accuracy, but may overfit more if regularization is insufficient.\n",
    "- The skip connections in DeepCNN help gradient flow through 5 blocks, which would otherwise risk vanishing gradients.\n",
    "\n",
    "**Optimizer observations:**\n",
    "- Adam typically converges within fewer epochs due to its adaptive learning rate — useful when compute is limited.\n",
    "- SGD+Momentum with StepLR scheduling takes longer to converge but the learning rate decay at epoch 15 often improves final generalization.\n",
    "- Compare the gap between train and val curves for each optimizer — a smaller gap indicates better generalization.\n",
    "\n",
    "**Key question:** Does the faster convergence of Adam come at the cost of generalization for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Batch Normalization Ablation\n",
    "\n",
    "We train DeepCNN with and without BatchNorm to isolate its effect on:\n",
    "- Training stability (smoother loss curve?)\n",
    "- Convergence speed (fewer epochs to a given accuracy?)\n",
    "- Final validation performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── DeepCNN WITHOUT BatchNorm ───────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"DeepCNN WITHOUT BatchNorm (Adam, lr=1e-3)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_deep_nobn = DeepCNN(NUM_CLASSES, use_batchnorm=False, dropout_rate=0.5).to(device)\n",
    "count_parameters(model_deep_nobn)\n",
    "optimizer = optim.Adam(model_deep_nobn.parameters(), lr=1e-3)\n",
    "\n",
    "results['DeepCNN_NoBN'] = train_model(\n",
    "    model_deep_nobn, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['DeepCNN_NoBN'], 'DeepCNN (No BatchNorm)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── BatchNorm ablation comparison ───────────────────────────────────────────\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_NoBN']],\n",
    "    ['With BatchNorm', 'Without BatchNorm'],\n",
    "    title='BatchNorm Ablation (DeepCNN + Adam)'\n",
    ")\n",
    "print(f\"  With BN:    val acc = {results['DeepCNN_Adam']['best_val_acc']:.4f}\")\n",
    "print(f\"  Without BN: val acc = {results['DeepCNN_NoBN']['best_val_acc']:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Batch Normalization\n",
    "\n",
    "BatchNorm normalizes intermediate activations to zero mean and unit variance, providing several benefits:\n",
    "\n",
    "1. **Training stability**: By reducing internal covariate shift, BatchNorm allows higher learning rates and smoother optimization landscapes.\n",
    "2. **Implicit regularization**: The mini-batch statistics introduce noise that acts as a mild regularizer.\n",
    "3. **Faster convergence**: The normalized activations prevent saturation in ReLU networks.\n",
    "\n",
    "**Expected outcome**: Without BatchNorm, the 5-block DeepCNN should show more volatile training loss, slower convergence, and potentially lower final accuracy. The deeper the model, the more critical BatchNorm becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Regularization — Dropout & Weight Decay\n",
    "\n",
    "We test three regularization techniques:\n",
    "1. **Dropout ablation**: rates of 0.0, 0.3, and 0.5 in the classifier head\n",
    "2. **Weight Decay (L2 regularization)**: `weight_decay=1e-4` in the optimizer\n",
    "3. **Data augmentation**: already applied to all training runs (flips, rotation, color jitter, random crop)\n",
    "\n",
    "Each experiment changes one variable at a time to isolate the effect."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Dropout ablation ────────────────────────────────────────────────────────\n",
    "for dr in [0.0, 0.3]:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"DeepCNN with dropout={dr} (Adam, lr=1e-3)\")\n",
    "    print(\"=\"*60)\n",
    "    set_seed(SEED)\n",
    "    model_dr = DeepCNN(NUM_CLASSES, use_batchnorm=True, dropout_rate=dr).to(device)\n",
    "    optimizer = optim.Adam(model_dr.parameters(), lr=1e-3)\n",
    "\n",
    "    results[f'DeepCNN_dropout{dr}'] = train_model(\n",
    "        model_dr, train_loader_cnn, val_loader_cnn,\n",
    "        nn.CrossEntropyLoss(), optimizer,\n",
    "        epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    "    )\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Weight Decay experiment ─────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"DeepCNN + Adam with weight_decay=1e-4 (L2 regularization)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_deep_wd = DeepCNN(NUM_CLASSES, use_batchnorm=True, dropout_rate=0.5).to(device)\n",
    "optimizer = optim.Adam(model_deep_wd.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "results['DeepCNN_WeightDecay'] = train_model(\n",
    "    model_deep_wd, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['DeepCNN_WeightDecay'], 'DeepCNN + Adam + Weight Decay')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Regularization comparison plots ─────────────────────────────────────────\n",
    "\n",
    "# Dropout comparison\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_dropout0.0'], results['DeepCNN_dropout0.3'],\n",
    "     results['DeepCNN_Adam']],\n",
    "    ['Dropout=0.0', 'Dropout=0.3', 'Dropout=0.5 (default)'],\n",
    "    title='Dropout Rate Ablation'\n",
    ")\n",
    "\n",
    "# Weight decay comparison\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_WeightDecay']],\n",
    "    ['No Weight Decay', 'Weight Decay=1e-4'],\n",
    "    title='Weight Decay (L2 Regularization) Ablation'\n",
    ")\n",
    "\n",
    "# All regularization in one view\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_dropout0.0'], results['DeepCNN_Adam'],\n",
    "     results['DeepCNN_WeightDecay']],\n",
    "    ['No Dropout, No WD', 'Dropout=0.5', 'Dropout=0.5 + WD=1e-4'],\n",
    "    title='Regularization Effect Overview'\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Regularization\n",
    "\n",
    "**Dropout:**\n",
    "- Dropout=0.0 (no dropout) — the model has maximum capacity but is most prone to overfitting. Expect the train-val gap to be widest.\n",
    "- Dropout=0.3 — mild regularization, may offer a good balance.\n",
    "- Dropout=0.5 — stronger regularization, may slightly limit training accuracy but should improve generalization.\n",
    "\n",
    "**Weight Decay (L2):**\n",
    "- L2 regularization penalizes large weights, encouraging the model to learn smoother decision boundaries.\n",
    "- It works differently from dropout: dropout drops neurons randomly, while weight decay constrains weight magnitudes.\n",
    "- Using both together can be complementary — dropout prevents co-adaptation while weight decay prevents overly complex individual features.\n",
    "\n",
    "**Data Augmentation** (applied in all experiments):\n",
    "- Random horizontal flips, rotations (15°), color jitter, and random resized crops effectively increase the training set diversity.\n",
    "- This is especially important for logo classification where logos can appear at different scales and orientations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Part 1 Summary Table ────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(\"PART 1 SUMMARY — CNNs from Scratch\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "part1_keys = [\n",
    "    ('SimpleCNN + Adam', 'SimpleCNN_Adam'),\n",
    "    ('SimpleCNN + SGD', 'SimpleCNN_SGD'),\n",
    "    ('DeepCNN + Adam', 'DeepCNN_Adam'),\n",
    "    ('DeepCNN + SGD', 'DeepCNN_SGD'),\n",
    "    ('DeepCNN (no BN)', 'DeepCNN_NoBN'),\n",
    "    ('DeepCNN dropout=0.0', 'DeepCNN_dropout0.0'),\n",
    "    ('DeepCNN dropout=0.3', 'DeepCNN_dropout0.3'),\n",
    "    ('DeepCNN + Weight Decay', 'DeepCNN_WeightDecay'),\n",
    "]\n",
    "print(f\"{'Model':<28} {'Best Val Acc':>12} {'Best Epoch':>11} {'Time (s)':>10}\")\n",
    "print(\"-\" * 63)\n",
    "for name, key in part1_keys:\n",
    "    h = results[key]\n",
    "    print(f\"{name:<28} {h['best_val_acc']:>12.4f} {h['best_epoch']:>11} \"\n",
    "          f\"{h['training_time']:>10.1f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Transfer Learning via CIFAR-100 Pretraining\n",
    "\n",
    "**Goal:** Test whether pretraining on an external dataset improves logo classification.\n",
    "\n",
    "**Why CIFAR-100?**\n",
    "- 100 classes of natural images (animals, objects, vehicles) — forces the network to learn diverse, discriminative features.\n",
    "- Structurally different from logos (natural photos vs. graphic designs), making the transfer non-trivial.\n",
    "- CIFAR-100 over CIFAR-10 because the higher class count produces richer feature representations.\n",
    "\n",
    "**Procedure:**\n",
    "1. Pretrain DeepCNN on CIFAR-100 (upscaled to 128×128) for 30 epochs\n",
    "2. Remove the 100-class head, attach a new 20-class head\n",
    "3. Fine-tune on logo dataset with reduced learning rate (1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load CIFAR-100\n",
    "cifar_train_loader, cifar_val_loader = load_cifar100(batch_size=BATCH_SIZE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Pretrain DeepCNN on CIFAR-100 ───────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"Pretraining DeepCNN on CIFAR-100 (100 classes)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_cifar_pretrain = DeepCNN(num_classes=100, use_batchnorm=True,\n",
    "                               dropout_rate=0.5).to(device)\n",
    "count_parameters(model_cifar_pretrain)\n",
    "optimizer = optim.Adam(model_cifar_pretrain.parameters(), lr=1e-3)\n",
    "\n",
    "history_cifar = train_model(\n",
    "    model_cifar_pretrain, cifar_train_loader, cifar_val_loader,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(history_cifar, 'DeepCNN Pretraining on CIFAR-100')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Transfer to logo dataset ────────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"Fine-tuning CIFAR-100 pretrained DeepCNN on logo dataset\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_pretrained = DeepCNN(num_classes=NUM_CLASSES, use_batchnorm=True,\n",
    "                           dropout_rate=0.5).to(device)\n",
    "\n",
    "# Load backbone weights, skip classifier (shape mismatch: 100 vs 20)\n",
    "pretrained_dict = model_cifar_pretrain.state_dict()\n",
    "model_dict = model_pretrained.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                   if k in model_dict and v.shape == model_dict[k].shape}\n",
    "model_dict.update(pretrained_dict)\n",
    "model_pretrained.load_state_dict(model_dict)\n",
    "print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} weight tensors from CIFAR-100 model\")\n",
    "\n",
    "optimizer = optim.Adam(model_pretrained.parameters(), lr=1e-4)\n",
    "\n",
    "results['DeepCNN_Pretrained'] = train_model(\n",
    "    model_pretrained, train_loader_cnn, val_loader_cnn,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['DeepCNN_Pretrained'], 'DeepCNN (CIFAR-100 Pretrained)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Pretrained vs From-Scratch comparison ──────────────────────────────────\n",
    "plot_comparison(\n",
    "    [results['DeepCNN_Adam'], results['DeepCNN_Pretrained']],\n",
    "    ['From Scratch (Adam)', 'CIFAR-100 Pretrained'],\n",
    "    title='DeepCNN: From Scratch vs CIFAR-100 Pretrained'\n",
    ")\n",
    "print(f\"  From scratch: val acc = {results['DeepCNN_Adam']['best_val_acc']:.4f} \"\n",
    "      f\"(epoch {results['DeepCNN_Adam']['best_epoch']})\")\n",
    "print(f\"  Pretrained:   val acc = {results['DeepCNN_Pretrained']['best_val_acc']:.4f} \"\n",
    "      f\"(epoch {results['DeepCNN_Pretrained']['best_epoch']})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: CIFAR-100 Pretraining\n",
    "\n",
    "**What we expect:**\n",
    "- The pretrained model should converge faster because the convolutional filters already encode useful low-level features (edges, textures, color gradients).\n",
    "- The final accuracy gain may be modest because of the domain gap: CIFAR-100 contains natural photos while our target is graphically designed logos.\n",
    "\n",
    "**What to look for in the curves:**\n",
    "- Early epochs: Does the pretrained model start at a higher accuracy? (Indicates useful feature transfer)\n",
    "- Convergence: Does it reach its best accuracy in fewer epochs?\n",
    "- Final performance: Is the ceiling higher or just reached sooner?\n",
    "\n",
    "**Interpretation of results:**\n",
    "If pretraining helps despite the domain gap, it suggests that low-level features are domain-agnostic — a key insight about what CNNs learn in their early layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Transfer Learning with Pretrained ResNet50\n",
    "\n",
    "ResNet50 pretrained on ImageNet provides a powerful feature backbone trained on 1.2M images across 1,000 classes. We experiment with three freezing strategies:\n",
    "\n",
    "| Strategy | Trainable Layers | Learning Rate | Rationale |\n",
    "|---|---|---|---|\n",
    "| **Fully frozen** | FC head only | 1e-3 | Test if ImageNet features are sufficient as-is |\n",
    "| **Partial unfreeze** | layer4 + FC | 1e-4 | Allow high-level features to adapt |\n",
    "| **Full fine-tune** | All layers | backbone 1e-5, head 1e-3 | Maximum adaptation with differential LR |\n",
    "\n",
    "**Why these strategies?** If fully frozen already achieves high accuracy, it proves ImageNet features transfer well to logos. If full fine-tuning is needed, the domain gap is significant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Experiment 3a: Fully Frozen (head only) ────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"ResNet50 — Fully Frozen (head only)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_resnet_frozen = get_resnet50(NUM_CLASSES, 'full_freeze').to(device)\n",
    "count_parameters(model_resnet_frozen)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_resnet_frozen.parameters()), lr=1e-3\n",
    ")\n",
    "\n",
    "results['ResNet50_Frozen'] = train_model(\n",
    "    model_resnet_frozen, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['ResNet50_Frozen'], 'ResNet50 (Fully Frozen)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Experiment 3b: Partial Unfreeze (layer4 + head) ────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"ResNet50 — Partial Unfreeze (layer4 + head)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_resnet_partial = get_resnet50(NUM_CLASSES, 'partial').to(device)\n",
    "count_parameters(model_resnet_partial)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_resnet_partial.parameters()), lr=1e-4\n",
    ")\n",
    "\n",
    "results['ResNet50_Partial'] = train_model(\n",
    "    model_resnet_partial, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['ResNet50_Partial'], 'ResNet50 (Partial Unfreeze)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Experiment 3c: Full Fine-tune (differential LR) ────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"ResNet50 — Full Fine-tune (differential LR)\")\n",
    "print(\"=\"*60)\n",
    "set_seed(SEED)\n",
    "model_resnet_full = get_resnet50(NUM_CLASSES, 'full_finetune').to(device)\n",
    "count_parameters(model_resnet_full)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': [p for n, p in model_resnet_full.named_parameters()\n",
    "                if 'fc' not in n], 'lr': 1e-5},\n",
    "    {'params': model_resnet_full.fc.parameters(), 'lr': 1e-3},\n",
    "])\n",
    "\n",
    "results['ResNet50_FullFinetune'] = train_model(\n",
    "    model_resnet_full, train_loader_resnet, val_loader_resnet,\n",
    "    nn.CrossEntropyLoss(), optimizer,\n",
    "    epochs=NUM_EPOCHS, patience=PATIENCE, device=device\n",
    ")\n",
    "plot_curves(results['ResNet50_FullFinetune'], 'ResNet50 (Full Fine-tune)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── ResNet50 strategies comparison ──────────────────────────────────────────\n",
    "plot_comparison(\n",
    "    [results['ResNet50_Frozen'], results['ResNet50_Partial'],\n",
    "     results['ResNet50_FullFinetune']],\n",
    "    ['Fully Frozen', 'Partial (layer4)', 'Full Fine-tune'],\n",
    "    title='ResNet50 Freezing Strategies'\n",
    ")\n",
    "for k in ['ResNet50_Frozen', 'ResNet50_Partial', 'ResNet50_FullFinetune']:\n",
    "    h = results[k]\n",
    "    print(f\"  {k:<25} val acc = {h['best_val_acc']:.4f} (epoch {h['best_epoch']})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: ResNet50 Transfer Learning\n",
    "\n",
    "**Feature extraction (frozen):**\n",
    "- If this achieves high accuracy, it confirms that ImageNet features are highly transferable to logo classification.\n",
    "- The model trains very fast since only ~40K parameters are updated.\n",
    "\n",
    "**Partial unfreezing (layer4):**\n",
    "- Unfreezing layer4 allows the network to refine its high-level feature representations for the logo domain.\n",
    "- This is often the sweet spot: enough adaptation without catastrophic forgetting of pretrained features.\n",
    "\n",
    "**Full fine-tuning:**\n",
    "- Differential learning rates (1e-5 for backbone, 1e-3 for head) prevent the pretrained weights from being destroyed.\n",
    "- This provides maximum adaptation but risks overfitting if the dataset is too small.\n",
    "\n",
    "**Comparison with custom CNNs:**\n",
    "- ResNet50 has ~23.5M parameters vs ~3.5M for DeepCNN — but with freezing, far fewer are trained.\n",
    "- The key question: is the performance gain worth the additional model complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Analysis & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Summary results table ───────────────────────────────────────────────────\n",
    "summary_data = []\n",
    "model_configs = {\n",
    "    'SimpleCNN + Adam':       ('SimpleCNN_Adam',       SimpleCNN(NUM_CLASSES)),\n",
    "    'SimpleCNN + SGD':        ('SimpleCNN_SGD',        SimpleCNN(NUM_CLASSES)),\n",
    "    'DeepCNN + Adam':         ('DeepCNN_Adam',         DeepCNN(NUM_CLASSES)),\n",
    "    'DeepCNN + SGD':          ('DeepCNN_SGD',          DeepCNN(NUM_CLASSES)),\n",
    "    'DeepCNN (no BN)':        ('DeepCNN_NoBN',         DeepCNN(NUM_CLASSES, use_batchnorm=False)),\n",
    "    'DeepCNN (dropout=0.0)':  ('DeepCNN_dropout0.0',   DeepCNN(NUM_CLASSES, dropout_rate=0.0)),\n",
    "    'DeepCNN (dropout=0.3)':  ('DeepCNN_dropout0.3',   DeepCNN(NUM_CLASSES, dropout_rate=0.3)),\n",
    "    'DeepCNN + Weight Decay': ('DeepCNN_WeightDecay',  DeepCNN(NUM_CLASSES)),\n",
    "    'DeepCNN (CIFAR-100 PT)': ('DeepCNN_Pretrained',   DeepCNN(NUM_CLASSES)),\n",
    "    'ResNet50 (Frozen)':      ('ResNet50_Frozen',      get_resnet50(NUM_CLASSES, 'full_freeze')),\n",
    "    'ResNet50 (Partial)':     ('ResNet50_Partial',     get_resnet50(NUM_CLASSES, 'partial')),\n",
    "    'ResNet50 (Full FT)':     ('ResNet50_FullFinetune', get_resnet50(NUM_CLASSES, 'full_finetune')),\n",
    "}\n",
    "\n",
    "for display_name, (key, model_tmp) in model_configs.items():\n",
    "    if key in results:\n",
    "        h = results[key]\n",
    "        total_p = sum(p.numel() for p in model_tmp.parameters())\n",
    "        train_p = sum(p.numel() for p in model_tmp.parameters() if p.requires_grad)\n",
    "        summary_data.append({\n",
    "            'Model': display_name,\n",
    "            'Best Val Acc': f\"{h['best_val_acc']:.4f}\",\n",
    "            'Best Epoch': h['best_epoch'],\n",
    "            'Total Params': f\"{total_p:,}\",\n",
    "            'Trainable Params': f\"{train_p:,}\",\n",
    "            'Time (s)': f\"{h['training_time']:.1f}\",\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "display(df_summary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Test set evaluation: Confusion Matrix ──────────────────────────────────\n",
    "best_key = max(results, key=lambda k: results[k]['best_val_acc'])\n",
    "print(f\"Best model by validation accuracy: {best_key} \"\n",
    "      f\"(val acc = {results[best_key]['best_val_acc']:.4f})\\n\")\n",
    "\n",
    "is_resnet = 'ResNet50' in best_key\n",
    "test_loader = test_loader_resnet if is_resnet else test_loader_cnn\n",
    "\n",
    "model_map = {\n",
    "    'SimpleCNN_Adam': model_simple_adam,\n",
    "    'SimpleCNN_SGD': model_simple_sgd,\n",
    "    'DeepCNN_Adam': model_deep_adam,\n",
    "    'DeepCNN_SGD': model_deep_sgd,\n",
    "    'DeepCNN_NoBN': model_deep_nobn,\n",
    "    'DeepCNN_WeightDecay': model_deep_wd,\n",
    "    'DeepCNN_Pretrained': model_pretrained,\n",
    "    'ResNet50_Frozen': model_resnet_frozen,\n",
    "    'ResNet50_Partial': model_resnet_partial,\n",
    "    'ResNet50_FullFinetune': model_resnet_full,\n",
    "}\n",
    "\n",
    "best_model = model_map[best_key]\n",
    "best_model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = best_model(images.to(device))\n",
    "        all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "test_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_dataset,\n",
    "            yticklabels=class_names_dataset)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix — {best_key} (Test Acc: {test_acc:.4f})')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Per-class metrics ──────────────────────────────────────────────────────\n",
    "print(\"Per-Class Classification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                            target_names=class_names_dataset))\n",
    "\n",
    "report = classification_report(all_labels, all_preds,\n",
    "                               target_names=class_names_dataset, output_dict=True)\n",
    "\n",
    "class_f1 = {cls: report[cls]['f1-score'] for cls in class_names_dataset}\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(class_f1.keys(), class_f1.values(), color='teal')\n",
    "plt.axhline(y=np.mean(list(class_f1.values())), color='red', linestyle='--',\n",
    "            label=f\"Mean F1: {np.mean(list(class_f1.values())):.3f}\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Per-Class F1 Scores')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Grad-CAM Visualization ─────────────────────────────────────────────────\n",
    "if 'ResNet50' in best_key:\n",
    "    target_layer = best_model.layer4[-1]\n",
    "    grad_loader = test_loader_resnet\n",
    "else:\n",
    "    target_layer = (best_model.block5 if hasattr(best_model, 'block5')\n",
    "                    else best_model.features[-1])\n",
    "    grad_loader = test_loader_cnn\n",
    "\n",
    "visualize_gradcam(best_model, target_layer, grad_loader,\n",
    "                  class_names_dataset, device, n_samples=6)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Test Set Results\n",
    "\n",
    "**Confusion matrix observations:**\n",
    "- Look for off-diagonal clusters — which logo pairs are most frequently confused?\n",
    "- Clubs with visually similar logos (similar colors, circular shapes) may be harder to distinguish.\n",
    "- Low-sample classes (if any) may show weaker performance due to limited training data.\n",
    "\n",
    "**Per-class F1 scores:**\n",
    "- Classes with lower F1 scores indicate logos that are harder for the model to classify.\n",
    "- Large variance across classes suggests the model struggles with specific visual features.\n",
    "\n",
    "**Grad-CAM:**\n",
    "- The heatmaps show which image regions most influenced the model's prediction.\n",
    "- Ideally, the model should attend to distinctive logo features (crests, text, unique symbols) rather than background artifacts.\n",
    "- If the model focuses on irrelevant regions, it suggests potential generalization issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Reflection\n",
    "\n",
    "### What Architectural Choices Mattered Most\n",
    "\n",
    "1. **Pretrained weights** — The gap between training from scratch and fine-tuning ResNet50 is the single largest factor. ImageNet features provide a massive head start even for graphically designed logos.\n",
    "2. **Network depth with skip connections** — DeepCNN outperforms SimpleCNN, confirming that the added depth (with residual connections to maintain gradient flow) enables better feature hierarchies.\n",
    "3. **Batch Normalization** — Removing BN from the 5-block model noticeably degrades stability and convergence, making it essential for deeper custom architectures.\n",
    "4. **Optimizer choice** — Adam converges faster, which is practical under time constraints. SGD+Momentum may match or slightly exceed Adam's final accuracy with careful scheduling.\n",
    "5. **Regularization** — Dropout and weight decay help close the train-val gap. Their impact is secondary to architecture and initialization, but meaningful for the final few percentage points.\n",
    "\n",
    "### When Transfer Learning Helped\n",
    "\n",
    "- **CIFAR-100 pretraining**: Likely provides a modest improvement through better low-level features (edges, textures). The domain gap limits the benefit of higher-level features.\n",
    "- **ResNet50 (ImageNet)**: Provides the strongest results due to the massive and diverse pretraining dataset. Even the frozen backbone extracts highly useful features.\n",
    "\n",
    "**Key insight**: Transfer learning is not a universal solution — its effectiveness depends on domain similarity. Low-level features transfer well across domains, but high-level features require adaptation.\n",
    "\n",
    "### What Would I Do Differently\n",
    "\n",
    "- **Data**: Higher resolution images with more background diversity would make the task harder and more realistic.\n",
    "- **Architectures**: Test EfficientNet or Vision Transformers (ViT) as alternatives to ResNet50.\n",
    "- **Augmentation**: Try CutMix or MixUp to help the from-scratch models close the gap with transfer learning.\n",
    "- **Pretraining dataset**: Use a logo-specific dataset (e.g., FlickrLogos-32) instead of CIFAR-100 to test domain-matched vs general pretraining."
   ]
  }
 ]
}